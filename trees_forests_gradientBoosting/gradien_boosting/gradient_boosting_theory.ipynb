{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88411b3-81b7-40c4-85ed-0735a1358dc1",
   "metadata": {},
   "source": [
    "# Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc1508-4e26-447e-8084-6b6c077884bc",
   "metadata": {},
   "source": [
    ">Градиентный бустинг — это семейство ансамблевых методов машинного обучения, которое строит модель путем последовательного обучения слабых моделей (обычно деревьев решений). Каждая следующая модель минимизирует ошибки (остатки) предыдущих моделей, постепенно улучшая качество предсказаний. Итоговое предсказание — это сумма предсказаний всех слабых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b420f-e025-48ca-a31d-764d70d15000",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"\" height=\"479\" name=\"image.png\" src=\"https://ucarecdn.com/f3e99e78-c62d-49c5-bf88-6f267f00c486/\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d64f82-6f44-4991-bcc5-a1e1dfc46d70",
   "metadata": {},
   "source": [
    "## Алгоритм обучения градиентного бустинга - регрессора (слегка упрощенный)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08240ed9-64a8-4b91-b5db-1a85010dbe97",
   "metadata": {},
   "source": [
    "##### 1. Инициализация модели:\n",
    "\n",
    "    Начнём с того, что предскажем среднее значение целевой переменной $y$ для всех объектов. Это будет наш первый прогноз $F_0 (x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1387d3-4343-476a-95e1-14fc4069f7c4",
   "metadata": {},
   "source": [
    "##### 2. Расчёт ошибки (остатка):\n",
    "\n",
    "- Для каждого объекта вычислим ошибку (остаток) на основе разницы между истинным значением $y$ и текущим прогнозом модели $F_m(x)$.\n",
    "- Остаток $r_i=y_i−F_m(x_i)$, где $m$ — номер текущей итерации, а $i$ — индекс объекта.\n",
    "- Заметим, что остаток $r_i=y_i−F_m(x_i)$ эквивалентен производной квадратичной функции потерь $L(y_i,F(x_i))=\\frac{1}{2} (y_i−F(x_i))^2$ (мы добавили константу по двум причинам: просто потому, что можем и потому, что это приводит к красивому результату; это ничего не меняет). $\\frac {\\partial L}{\\partial F(x_i)}=−(y_i−F(x_i))$. В будущем мы можем обозначать остаток через производную (градиент для многих строк).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba65e3a-10c9-4a15-9918-e4f29aa6a61d",
   "metadata": {},
   "source": [
    "##### 3. Обучение слабого ученика (дерева решений):\n",
    "\n",
    "Построим маленькое дерево решений (обычно с небольшой глубиной, чтобы оно не переобучилось; к тому же нет необходимости в глубоких деревьях, тк все равно каждое следующее дерево улучшает результат предыдущего) для предсказания этих ошибок (остатков). Слабый ученик будет обучен на предсказание этих ошибок, чтобы улучшить текущую модель.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6dec0-80d7-4f36-8558-bd5ed797444f",
   "metadata": {},
   "source": [
    "##### 4. Обновление прогноза:\n",
    "\n",
    "Обновим текущий прогноз: новый прогноз $F_{m+1}(x)$ = текущий прогноз $F_m(x)$ + маленькая доля предсказаний нового дерева, умноженная на коэффициент скорости обучения $\\eta$.\n",
    "    $F_{m+1}(x)=F_m(x)+\\eta \\cdot T_m(x)$, где $T_m(x)$ — предсказания нового дерева.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db09097-a1b9-4a3d-b014-b747fd1edcb2",
   "metadata": {},
   "source": [
    "##### 5. Повторение итераций:\n",
    "\n",
    "Повторяем шаги 2-4 для каждой итерации (m). На каждой итерации добавляется новое дерево, и обновляется общий прогноз модели.\n",
    "    Итерации продолжаются до тех пор, пока модель не станет достаточно точной или пока не будет достигнуто заданное количество итераций.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa25fdc-18bf-4015-bf30-d55e737a48fb",
   "metadata": {},
   "source": [
    "### Пример итераций:\n",
    "\n",
    "- **Шаг 1**: Начальный прогноз — среднее значение целевой переменной.\n",
    "- **Шаг 2**: Вычисляем ошибку для каждого объекта.\n",
    "- **Шаг 3**: Обучаем слабое дерево на ошибках.\n",
    "- **Шаг 4**: Обновляем текущий прогноз с учётом предсказаний нового дерева.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a027bc7-bfb8-4763-82d7-7c14378b9f2d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"\" height=\"297\" name=\"image.png\" src=\"https://ucarecdn.com/db442eda-7ead-44d7-99b1-ce3e97d0b429/\" width=\"392\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e6d8d-a550-4ba2-9dbf-bf66ad2b909a",
   "metadata": {},
   "source": [
    "В случае алгоритма классификации всё очень похоже за исключением того, что каждое следующее дерево корректирует значение вероятности (логитов) принадлежности объектов к определенному классу. Деревья решений в этом алгоритме не предсказывают классы напрямую, а вычисляют градиенты логистической функции потерь и корректируют предсказания вероятностей. Остатки (разности) в этом случае эквивалентны градиентам, и это позволяет модели на каждом шаге улучшать свои предсказания, делая их всё более точными."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79399522-243f-4d5f-9633-456be78da2d6",
   "metadata": {},
   "source": [
    "Вместо того чтобы параллельно строить случайные независимые варианты дерева решений, GB - это последовательный метод, который направлен на улучшение производительности каждого последующего дерева. Это происходит за счет оценки эффективности слабых моделей и последующего повышения веса последующих моделей для смягчения результатов случаев, неправильно классифицированных в предыдущих раундах. Экземпляры, правильно классифицированные в предыдущем раунде, заменяются более высокой долей экземпляров, которые не были точно классифицированы.\n",
    "\n",
    "Хотя это создает еще одну слабую модель, модификации, полученные от предыдущей модели, помогают новой модели учесть ошибки предыдущего дерева. Способность алгоритма учиться на своих ошибках делает градиентное усиление одним из самых популярных алгоритмов в машинном обучении на сегодняшний день."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eadc7a5-fa27-411f-88ee-6e1ad1891f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
