{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234f5739-3829-40e9-9d05-83247cf17066",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428134b2-058d-4e9c-9a69-1d05defc92f7",
   "metadata": {},
   "source": [
    "#### концепция баланса между смещением (bias) и дисперсией (variance).\n",
    "\n",
    ">    Смещение (bias): Отражает, насколько сильно модель упрощает зависимость. Высокое смещение характерно для моделей с низкой сложностью, которые не могут адекватно описать сложные зависимости в данных. Смещение  — это систематическая ошибка модели, которая возникает из-за её неспособности точно отразить сложные зависимости в данных. Это приводит к тому, что предсказания модели регулярно отклоняются от истинных значений в одну сторону, что связано с чрезмерным упрощением (или недообучением).\n",
    "\n",
    ">    Дисперсия (variance): Характеризует, насколько модель чувствительна к изменениям в обучающей выборке. Высокая дисперсия свойственна сложным моделям, которые могут слишком точно подогнать данные и переобучиться.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079a3f2-8e71-49be-b3ac-762fd53c9ff6",
   "metadata": {},
   "source": [
    "```Переобучение возникает тогда, когда модель имеет низкое смещение, но высокую дисперсию — она слишком хорошо подстраивается под обучающие данные, но плохо справляется с новыми.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff8334-d040-4ffe-ba8a-c66cebc892fd",
   "metadata": {},
   "source": [
    "## Необходимость регуляризации\n",
    "\n",
    "Регуляризация — это важный метод борьбы с переобучением, который помогает уменьшить дисперсию модели, не увеличивая её смещение слишком сильно. Основная идея регуляризации заключается в том, чтобы контролировать сложность модели, предотвращая её чрезмерную адаптацию к обучающей выборке. Регуляризация вводит дополнительные ограничения на параметры модели, что помогает снизить вероятность переобучения.\n",
    "\n",
    "В математическом смысле регуляризация заключается в добавлении дополнительных членов в функцию потерь, которые штрафуют модель за слишком большие значения параметров. Это заставляет модель выбирать более \"простые\" решения, которые лучше обобщают данные.\n",
    "\n",
    "Большие веса делают модель слишком зависимой от отдельных признаков, усиливая их влияние на предсказания. Это приводит к тому, что модель лучше запоминает обучающую выборку, включая её шум, вместо того, чтобы находить общие закономерности, что и вызывает переобучение.\n",
    "\n",
    "Регуляризация штрафует за слишком большие веса!\n",
    "\n",
    "Регуляризация особенно полезна, когда модель имеет много параметров или когда доступно ограниченное количество данных. В таких случаях регуляризация помогает снизить риск того, что модель начнёт подстраиваться под случайные шумы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b0c91-7656-48c2-a234-61281aeb76e4",
   "metadata": {},
   "source": [
    "## Обобщающая способность модели\n",
    "> **Обобщающая способность** описывает, насколько хорошо модель может предсказывать данные, которые она не видела во время обучения.\n",
    "> Основная цель **регуляризации** — улучшить обобщающую способность модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ada614-8c06-4035-b80b-21eefd69c44f",
   "metadata": {},
   "source": [
    "## Типы регуляризации в машинном обучении\n",
    "> Суть регуляризации заключается в добавлении штрафов к функции потерь модели, что предотвращает её излишнюю сложность и подгонку под случайные шумы в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce334a-8812-4cee-9731-df812a8ef0ae",
   "metadata": {},
   "source": [
    "## 1. Регуляризация L2 (Ridge)\n",
    "\n",
    "**L2-регуляризация** — это один из самых популярных методов регуляризации, который также известен как **гребневая регрессия (Ridge)**. В L2-регуляризации к функции потерь модели добавляется штраф, пропорциональный сумме квадратов весов параметров модели. Это позволяет ограничить значения параметров модели и предотвращает их чрезмерный рост.\n",
    "\n",
    "Основная идея L2-регуляризации заключается в том, что мы стремимся минимизировать не только ошибку модели, но и величину её параметров. Это предотвращает чрезмерную подгонку модели под данные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c990634-9984-40f8-992e-b7f9ced7b559",
   "metadata": {},
   "source": [
    "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\widehat{y_i})^2 + \\lambda \\sum_{j=1}^m \\theta^2_j$$\n",
    "$\\lambda$ — это коэффициент регуляризации, который контролирует силу регуляризации,\\\n",
    "$\\theta_j$ — это веса модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f63ffd-9e26-4773-811d-c7af29f8d533",
   "metadata": {},
   "source": [
    "#### **Преимущества:**\n",
    "\n",
    "- L2-регуляризация хорошо работает, когда все признаки вносят вклад в предсказание, но требуется контролировать их вес.\n",
    "\n",
    "- Способствует созданию модели, которая устойчива к шуму в данных.\n",
    "\n",
    "#### **Недостатки:**\n",
    "\n",
    "- Веса параметров уменьшаются, но никогда не обнуляются, что может быть неэффективно для задач с избыточными признаками.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be17970-2b7e-4427-9b0d-3642d8ccfc5c",
   "metadata": {},
   "source": [
    "## 2. Регуляризация L1 (Lasso)\n",
    "\n",
    "**L1-регуляризация** или **Lasso** (**Least Absolute Shrinkage and Selection Operator**) также добавляет штраф к функции потерь, но вместо квадратов параметров модель штрафуется за абсолютные значения весов. Это приводит к тому, что некоторые параметры могут стать равными нулю, effectively excluding certain features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e823fc2-5c00-491c-b40a-bb0eaf93e314",
   "metadata": {},
   "source": [
    "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\widehat{y_i})^2 + \\lambda \\sum_{j=1}^m |\\theta_j|$$\n",
    "$\\lambda$ — это коэффициент регуляризации, который контролирует силу регуляризации,\\\n",
    "$\\theta_j$ — это веса модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8429211-3fa0-4ccd-83e9-bb09f8a01bcc",
   "metadata": {},
   "source": [
    "```Важное отличие от L2-регуляризации заключается в том, что L1-регуляризация способствует занулению некоторых весов, что приводит к отбору признаков.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147bb83-8548-4d5f-bc09-ffb58210a0d5",
   "metadata": {},
   "source": [
    "#### **Преимущества:**\n",
    "\n",
    "- L1-регуляризация эффективно справляется с задачами, где есть избыточные или неинформативные признаки, так как она стремится занулять их вес.\n",
    "- Позволяет сделать модель более интерпретируемой за счёт сокращения числа используемых признаков.\n",
    "\n",
    "#### **Недостатки:**\n",
    "\n",
    "- L1-регуляризация может плохо работать в ситуациях, когда существует несколько признаков с похожими уровнями важности. В таких случаях она может случайно занулять один из полезных признаков.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966519a-73c9-4fdc-9d9b-38838673d4ea",
   "metadata": {},
   "source": [
    "## 3. Elastic Net\n",
    "\n",
    "**Elastic Net** — это метод регуляризации, который комбинирует L1 и L2-регуляризацию. Идея Elastic Net заключается в том, чтобы одновременно учитывать как штраф за абсолютные значения весов (L1), так и за их квадраты (L2). Это даёт более гибкую настройку модели, позволяя извлекать лучшее из обоих методов.\n",
    "$$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\widehat{y_i})^2 + \\lambda_1 \\sum_{j=1}^m |\\theta_j| + \\lambda_2 \\sum_{j=1}^m \\theta^2_j$$\n",
    "$\\lambda_1$ и $\\lambda_2$ — это коэффициенты регуляризации, которые контролируют долю каждого из штрафов.\\\n",
    "$\\theta_j$ — это веса модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b327c-eef1-4214-a7ba-0fce1c1eb338",
   "metadata": {},
   "source": [
    "#### **Преимущества:**\n",
    "\n",
    "- Сочетает преимущества L1 и L2-регуляризаций.\n",
    "- Работает лучше L1-регуляризации при наличии множества коррелированных признаков.\n",
    "\n",
    "#### **Недостатки:**\n",
    "\n",
    "- Требуется подбор двух гиперпараметров ($\\lambda_1$ и $\\lambda_2$), что усложняет настройку модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065349dc-c16c-4d89-a9c7-363f63e1e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
